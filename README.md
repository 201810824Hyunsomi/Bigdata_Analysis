# Bigdata_Analysis

##  대량의 데이터를 분석합니다. spark를 사용해 수집된 데이터를 기계학습으로 분류 및 예측합니다. 
    웹사이트에서 크롤링 하는 실습을 진행합니다. sns에서 데이터를 수집하는 방식으로 ouath인증과 rest api를 실습하고, 수집된
    데이터는 파일로 저장합니다. 대량의 데이터를 프로그래밍으로 수집, 정제, 저장, 분석, 시각화합니다. 

### 1주차 : pyspark 시작하기
### 2주차 : 경기도 의정부시 인구현황(https://www.data.go.kr/data/15009613/fileData.do)인구현황을 읽어와 rdd를 생성합니다. 
### 3주차 : 텍스트 파일을 읽고 pyspark으로 RDD를 생성하여, 단어빈도를 계산하는 프로그램,단어빈도를 내림차순으로 출력해서 상위 15개를 출력

### 4주차 : 데이터를 RDD로 만들고, 성적의 합계 및 평균을 계산하세요
### 5주차 : 서울시 열린데이터 https://data.seoul.go.kr/ 에서 제공하는 서울특별시_공공자전거 일별 대여건수_(2018~2019.03).csv를 분석(년도별 대여건수 합계,년도별, 월별 대여건수 합계,년도별, 월별 대여건수 그래프)
### 6주차 : zscore, cdf 계산


### midterm : 

   1. 2020년 8월 서울시 지하철호선별 역별 승하차 인원 정보(승하차 합계 가장높은 노선 출력, 노선별 요일별 인원합계)
   2. 한국의 코로나 바이러스(년/월별 건수 출력, 도별 첫 확진자 출현, 도별 누적확진자 출력)
   3. 키, 몸무게 상관관계(bmi컬럼 생성, weight와 height의 zscore컬럼 생성, zscore 활용해 상관관계 계산 , scipy를 사용 )
  
### 9주차 : 기념축사 TF-IDF
### 10주차클러스터링 : sklearn.datasets.make_blobs를 사용해 데이터 생성후 4개의 클러스터로 군집화함(클러스터 중심값 출력)
### 11주차 : matrix inverse method 회귀분석
### 12주차 : iris데이터를 사용해서 회귀분석(* (1) DataFrame 생성 (2) 데이터와 회귀선 그래프 (3) Spark를 사용하여 회귀분석하고 계수 출력 (4) 실제와 예측 출력 (4) 정확성 평가)
### 13주차 : Sklearn make_classification으로 생성된 데이터에 대해 이진 베이지안 모델
### 14주차 : 야구/배구 베이지안 분류
### 기말고사 : 회귀분석,클러스터링, 텍스트 분류
